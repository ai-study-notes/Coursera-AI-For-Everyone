When you work with AI teams, you may hear them refer to the tools that
they're using to build these AI systems. In this video,
I want to share with you some details and names of the most
commonly used AI tools so that you'll be able to better understand
what these AI engineers are doing. We're fortunate that the AI
world today is very open and many teams will openly share
idea years with each other. There are great machine learning,
open-source tools and frameworks that many teams will
use to build their systems. So if you hear any of these PyTorch,
TensorFlow, Hugging Face PaddlePaddle,
Scikit-Learn, or R. All of these are open source
machine learning tools or frameworks that help AI teams
be much more effective. A lot of AI technology breakthroughs
are also published freely on the Internet on this website
called archive, spelled like this. I hope that other academic communities
also freely share their research, since I've seen firsthand how much
this accelerates progress in the whole field of AI. Finally, many teams will also share
their code freely on the internet, most commonly on a website called GitHub. This has become the de
facto repository for open-source software in AI and
in other sectors in AI. And by using appropriately licensed
open-source software, many teams can get going much faster than if they
had to build everything from scratch. So, for example, if I search online for face recognition software on GitHub, you might find a web page like this. And if you scroll down,
this actually has a pretty good, very reasonable description of software
that is made available on this website. For recognizing people's faces and
even finding parts of people's faces. There's just a ton of software
that is really downloadable for doing all sorts of things on the internet
and just double check the license. Or AI team would double check the license
before using it in a product, of course. But a lot of the software
is open-source or is otherwise very permissively
licensed for anyone to use. Although GitHub is a technical website
built for engineers, if you want, you should feel free to
play around GitHub and see what other types of AI software
people have released online as well. In addition to these
open-source technical tools, you often also hear AI engineers
talk about CPUs and GPUs. Here's what these terms mean. A CPU is the computer processor in your
computer, whether it's your desktop, your laptop, or
a compute server off in the cloud. CPU stands for central processing unit,
and CPUs are made by intel and AMD and a few other companies, this does
a lot of the computation in your computer. GPU stands for graphics processing unit. Historically, the GPU was made to process
pictures, so if you play a video game, it's probably a GPU that is
drawing the fancy graphics. But what we found several years ago was
that the hardware that was originally built for processing graphics turns
out to be very, very powerful. For building very large neural networks or
very large deep learning algorithms. Given the need to build
very large deep learning or very large neural network systems. The AI community has had this
insatiable hunger for more and more computational power to train
bigger and bigger neural networks. And GPUs have proved to be a fantastic
fit to this type of computation that we need to have done to train
very large neural networks. So that's why gpus are playing a big
role in the rise of deep learning. Nvidia is a company that's been selling
many GPUs, but other companies, including Qualcomm,
as well as Google making its own TPUs. Are increasingly making
specialized hardware for powering these very large neural networks. Finally, you might hear about
cloud versus on-premises or, for short, on-prem deployments. Cloud deployments refer to
if you rent compute servers, such as from Amazon's AWS or
Microsoft's Euro or Google's GCP, in order to use someone else's
service to do your computation. Whereas an on-prem deployment means
buying your own compute service and running the service locally
in your own company. A detailed exploration of the pros and cons of these two options is
beyond the scope of this video. A lot of the world is moving to cloud
deployments, but if you search online, you find many articles
talking about the pros and cons of cloud versus on-prem deployments. There is one last term you might hear
about, which is edge deployments. If you are building a self-driving car,
there's not enough time to send data from a self driving car to a cloud server to
decide if you should stop the car or not. And then send that message
back to the self-driving car. So the computation has to happen usually
in a computer right there inside the car, that's called an edge deployment. Where you put a processor right
where the data is collected so that you can process the data and make
a decision very quickly without needing to transmit the data over the internet
to be processed somewhere else. If you look at some of the smart
speakers in your home as well, this too is an edge deployment,
where some, not all. But some of the speech recognition tasks
is done by a processor that is built in right there into the smart
speaker that is inside your home. The main advantage of edge deployments
is it can increase response time of the system and also reduce the amount of
data you need to send over the network. But there are many pros and cons as
well about edge versus cloud versus on-prem deployments that you can also
search online to read more about. Thanks for finishing this optional video on
the technical tools that AI engineers use. Hopefully when you hear them
refer to some of these tools, you'll start to have a better
sense of what they mean. I look forward to seeing you next week.