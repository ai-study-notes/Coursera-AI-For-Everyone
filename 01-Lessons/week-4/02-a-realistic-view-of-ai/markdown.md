# A Realistic View Of AI

AI is having a huge impact on society and
on so many people's lives. So for all of us to make good decisions,
it's important that we have a realistic view of AI and be neither
too optimistic nor too pessimistic. Here's what I mean. Did you ever read the story of
Goldilocks and the three bears? Maybe when you were a kid? Part of the story was that a bow of
porridge should be neither too hot nor too cold, and a bed should be
neither too firm nor too soft. I think we need a similar
Goldilocks rule for AI, where I think it's important that
we be neither too optimistic nor too pessimistic about what AI
technology can or cannot do. For example, we should not be too
optimistic about AI technologies. AI is a very powerful technology, but
I wouldn't expect it to solve all of humanity's problems and by itself usher in
some sort of global utopia for humanity. Some of the over optimism stems from
people thinking that AI sentience, artificial general intelligence, or even superintelligence might
be right around the corner. And that when we get there, AI will rapidly invent huge breakthroughs
in healthcare and give us all long and healthy lives, and also create an insane
amount of wealth for everyone. I wish it were that easy. On the flip side, some of the most
pessimistic fears about AI also relate to thinking superintelligence might
be right around the corner, and that AI might become sentient and decide
to, I don't know, kill of all humans. This seems extremely unlikely to me. Even though AI has risks, for
example, it can give bias, unfair or inaccurate outputs. Our losing control of
it to the point of AI becoming akin to a superior
species that then wipes us out, is really in the realm of science fiction
rather than a realistic scenario. Humanity has lots of experience
controlling things more powerful than any individual,
like corporations and nation states. And while AI's output is sometimes
unpredictable, I'm not worried about us so called losing control of AI or
becoming a competitive species to us. I think unnecessary fears and
overly optimistic hopes about sentience, superintelligence, artificial
general intelligence, are distracting people
from the real issues and is also causing unnecessary fears
about AI in parts of society. In contrast, I think a more realistic
view of AI is that it is a very, very powerful tool, but that there
are also many things that AI cannot do. It has some potential harms, like bias,
unfairness, and inaccurate outputs that we can mitigate, and it is already
creating tremendous economic value. Further, we see
a surprisingly clear path for it to continue to create even more
value in multiple industries. So I'm confident, and so are many other
AI system builders, that AI will continue to grow and continue to help more and
more people for the foreseeable future. To summarize, rather than being
too optimistic or too pessimistic, the lesson of Goldilocks is that taking
a realistic in between view is just right. When you speak with friends about AI,
I hope you can also tell them about this Goldilocks rule for AI so that they too
can have a more realistic view of AI. There are many limitations of AI. You have already seen earlier some
of the performance limitations, but AI has other limitations as well. One of the limitations of AI is
that explainability is hard, and many high performing AI
systems are black boxes. Meaning that it works very well but the AI doesn't know how to
explain why it does what it does. Here's an example, let's see you have
an AI system look at this x-ray image to diagnose if anything is
wrong with the patient. In this example, which is a row example, the AI system says that it thinks
the patient has right sided pneumothorax, which means that the right
lung is collapsed. But how do we know if the AI is right? And how do you know if you should trust
the AI system's diagnosis or not? There's been a lot of work on making
AI systems explain themselves. In this example, the heat map is
the AI telling us what part of the image it is looking at in
order to make this diagnosis, because it is clearly basing its
diagnosis on the right lung and in fact,
on some key features of the right lung. Seeing this image may give us
more confidence that the AI is making a reasonable diagnosis. Now, to be fair, humans are also not very
good at explaining how we make decisions ourselves. For example, you've already seen this
coffee mug in last week's videos. But how do you know it's a coffee mug? How does a human look at this and
say, that's a coffee mug? There are some things you can point to,
like there's a room for liquid and has a handle. But we humans are not very good
explaining how we can look at this and decide what it is. But because AI is a relatively new thing, the lack of explainability is
sometimes a barrier to its acceptance. And also sometimes if an AI system isn't
working, then its ability to explain itself would also help us figure how to
go in and make the AI system work better. So explainability is one of
the major open research areas. A lot of researchers are working on. What I see in practice is that when
an AI team wants to deploy something, the AI team is often able to come up
with an explanation that is good enough to enable the system to work and
be deployed. So explainability is hard,
but is often not impossible. But we do need much better tools to
help AI systems explain themselves. AI has some other serious limitations. As a society, we do not want to
discriminate against individuals based on their gender, based on their ethnicity,
and we want people to be treated fairly. But when AI systems are fed data
that doesn't reflect these values, then an AI can become biased or can learn
to discriminate against certain people. The AI community is working hard and is
making good progress on these issues, but we're far from done and
there's still a lot of work to do. You learn more about biased
AI in the next video, and some ideas how to make sure that AI
systems you work with are less biased. Finally, many AI systems are making
economically important decisions, and some AI systems are open to
adversarial attacks if someone else is deliberately out to
fool your AI system. So depending on your application,
it may be important to make sure that you are not open to these types
of attacks on your AI systems. The issues of AI and
discrimination, or AI and bias, as well as the issue of adversarial
attacks on AI, are important both to you as a potential builder and
user of AI, as well as to society. In the next video, let's dive more
deeply into the issue of AI and bias.

---

###### last update: 25-7-2025